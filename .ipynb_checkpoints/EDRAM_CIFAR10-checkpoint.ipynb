{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.transform import rescale, resize\n",
    "sys.path.append(os.path.abspath(\"./utils.py\"))\n",
    "%run utils.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "using GPU:  cuda:1\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "print('=================================')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    torch.cuda.set_device(2)\n",
    "    print('using GPU: ', device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print('using CPU')\n",
    "print('=================================')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "     transforms.Resize([100,100]),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, drop_last = True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=False, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, drop_last = True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_tensor(input_tensors, h, w):\n",
    "    final_output = None\n",
    "    batch_size, channel, height, width = input_tensors.shape\n",
    "    for img in input_tensors:\n",
    "        img = img.cpu().numpy()\n",
    "        img_PIL = resize(img, (channel,h,w))\n",
    "        img_PIL = torch.from_numpy(img_PIL)\n",
    "        img_PIL = torch.unsqueeze(img_PIL,0)\n",
    "        if final_output is None:\n",
    "            final_output = img_PIL\n",
    "        else:\n",
    "            final_output = torch.cat((final_output, img_PIL), 0)\n",
    "    final_output = final_output.to(device)\n",
    "    return final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x.requires_grad_(True)\n",
    "        return x.view(x.size(0), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GlimpseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlimpseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlimpseNet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size = (2,2), stride = (2,2)),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size = (2,2), stride = (2,2)),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(128, 160, kernel_size=3, padding = 1),\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Conv2d(160, 192, kernel_size=3),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer7_image = nn.Sequential(\n",
    "            Flatten(),\n",
    "            nn.Linear(3072, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU())\n",
    "\n",
    "        self.layer8_loc = nn.Sequential(\n",
    "            nn.Linear(6, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU())\n",
    "#         #initilzing weight and bias on all conv2d layers\n",
    "#         self.layer1[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer2[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer3[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer4[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer5[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer6[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer7_image[1].weight.data.normal_(std = 0.001)\n",
    "#         self.layer8_loc[0].weight.data.normal_(std = 0.001)\n",
    "#         self.layer1[0].bias.data.zero_()\n",
    "#         self.layer2[0].bias.data.zero_()\n",
    "#         self.layer3[0].bias.data.zero_()\n",
    "#         self.layer4[0].bias.data.zero_()\n",
    "#         self.layer5[0].bias.data.zero_()\n",
    "#         self.layer6[0].bias.data.zero_()\n",
    "#         self.layer7_image[1].bias.data.zero_()\n",
    "#         self.layer8_loc[0].bias.data.zero_()\n",
    "\n",
    "    def forward(self,input_tensors):\n",
    "        glimpse, theta = input_tensors\n",
    "        x = self.layer1(glimpse)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "\n",
    "        # Conv features\n",
    "        conv_features = self.layer7_image(x)\n",
    "\n",
    "        # location features\n",
    "\n",
    "        loc_features = self.layer8_loc(theta)\n",
    "\n",
    "        return conv_features * loc_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ContextNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextNet(nn.Module):\n",
    "    def __init__(self, size = 12):\n",
    "        super(ContextNet, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 5),\n",
    "            nn.BatchNorm2d(16), ## NoRNN version has a maxpooling here\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16,16,3),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(16,32,3),\n",
    "            nn.BatchNorm2d(32), ## NoRNN version has a maxpooling here\n",
    "            nn.ReLU(),\n",
    "            Flatten()\n",
    "        )\n",
    "\n",
    "#         self.layer1[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer2[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer3[0].weight.data.uniform_(-0.01,0.01)\n",
    "#         self.layer1[0].bias.data.zero_()\n",
    "#         self.layer2[0].bias.data.zero_()\n",
    "#         self.layer3[0].bias.data.zero_()\n",
    "\n",
    "    def forward(self, input_tensors):\n",
    "        resized_image = resize_tensor(input_tensors, 12, 12)\n",
    "        x = self.layer1(resized_image)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmissionNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmissionNet, self).__init__()\n",
    "        self.dense1 = nn.Linear(512, 32)\n",
    "        self.dense2 = nn.Linear(32,6)\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.dense2.weight.data.zero_()\n",
    "        self.dense2.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        # Initilize the weights and bias\n",
    "        x = self.dense1(input_tensor)\n",
    "        x = self.Relu(x)\n",
    "        x = self.dense2(x) # there is act functon after the last dense layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ClassificationNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(512,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        x = self.layer1(input_tensor)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RecurrentNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecurrentNet, self).__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.class_mlp = nn.Linear(1024, 512 * 4)\n",
    "        self.class_rnn = nn.LSTMCell(512*4, 512)\n",
    "\n",
    "        self.emission_mlp = nn.Linear(512, 512 * 4)\n",
    "        self.emission_rnn = nn.LSTMCell(512 * 4, 512)\n",
    "\n",
    "        #initilizing weight and bias for RNNs\n",
    "        self.class_rnn.weight_ih.data.uniform_(-0.01,0.01)\n",
    "        self.class_rnn.weight_hh.data.uniform_(-0.01,0.01)\n",
    "        self.class_rnn.bias_ih.data.fill_(0)\n",
    "        self.class_rnn.bias_hh.data.fill_(0)\n",
    "#         #initilizing weight and bias for dense layers\n",
    "#         self.class_mlp.weight.data.normal_(std = 0.001)\n",
    "#         self.emission_mlp.weight.data.normal_(std = 0.001)\n",
    "#         self.class_mlp.bias.data.zero_()\n",
    "#         self.emission_mlp.bias.data.zero_()\n",
    "\n",
    "        self.class_state = None\n",
    "        self.emission_state = None\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(batch_size, 512)\n",
    "        cell = torch.zeros(batch_size, 512)\n",
    "        hidden = hidden.to(device)\n",
    "        cell = cell.to(device)\n",
    "        return (hidden, cell)\n",
    "\n",
    "    def reset_states(self, conv_init):\n",
    "        x = torch.zeros_like(conv_init)\n",
    "        x = x.to(device)\n",
    "        self.class_state = self.init_hidden(conv_init.shape[0])\n",
    "        self.emission_state = (conv_init, x)\n",
    "\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        c = self.class_mlp(input_tensor)\n",
    "        hidden_class, cell_class = self.class_rnn(c, self.class_state)\n",
    "        c = hidden_class\n",
    "        self.class_state = (hidden_class, cell_class)\n",
    "        e = self.emission_mlp(c)\n",
    "        hidden_emission, cell_emission = self.emission_rnn(e, self.emission_state)\n",
    "        e = hidden_emission\n",
    "        self.emission_state = (hidden_emission, cell_emission)\n",
    "        return c, e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EDRAM(nn.Module):\n",
    "    def __init__(self, glimpse_size = 26, context_size = 12, batch_size = batch_size,\n",
    "              num_glimpse = 6):\n",
    "        super(EDRAM, self).__init__()\n",
    "        self.glimpse_size = glimpse_size\n",
    "        self.context_size = context_size\n",
    "        self.num_glimpse = num_glimpse\n",
    "\n",
    "        # Create individual models\n",
    "\n",
    "        self.glimpse_net = GlimpseNet()\n",
    "        self.context_net = ContextNet(context_size)\n",
    "        self.emission_net = EmissionNet()\n",
    "        self.class_net = ClassificationNet()\n",
    "        self.rnn_net = RecurrentNet()\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        probs_array = None\n",
    "        theta_array = None\n",
    "\n",
    "\n",
    "        # Note that, the images get resized to 12 * 12 on the contextNet\n",
    "        shape_input = input_tensor.shape\n",
    "#         input_tensor = input_tensor.view(shape_input[0],shape_input[3],shape_input[1],shape_input[2])\n",
    "        # context net\n",
    "        r0 = self.context_net(input_tensor)\n",
    "\n",
    "        self.rnn_net.reset_states(r0)\n",
    "\n",
    "        # Get initial glimpse theta\n",
    "        theta_ori = self.emission_net(r0)\n",
    "        theta = theta_ori.view(-1, 2, 3)\n",
    "        # Transform\n",
    "        grid = F.affine_grid(theta, [batch_size,3,self.glimpse_size,self.glimpse_size])\n",
    "        glimpse = F.grid_sample(input_tensor, grid)\n",
    "        \n",
    "        for i in range(self.num_glimpse):\n",
    "            glimpse_features = self.glimpse_net((glimpse, theta_ori))\n",
    "            r1, r2 = self.rnn_net(glimpse_features)\n",
    "            probs = self.class_net(r1)\n",
    "\n",
    "            probs = torch.unsqueeze(probs,0)\n",
    "            theta_temp = torch.unsqueeze(theta_ori, 0)\n",
    "            if probs_array is None and theta_array is None:\n",
    "                probs_array = probs\n",
    "                theta_array = theta_temp\n",
    "            else:\n",
    "                probs_array = torch.cat((probs_array, probs), dim = 0)\n",
    "                theta_array = torch.cat((theta_array, theta_temp), dim = 0)\n",
    "\n",
    "            if(i < (self.num_glimpse - 1)):\n",
    "                theta_ori = self.emission_net(r2)\n",
    "                theta = theta_ori.view(-1, 2, 3)\n",
    "                grid = F.affine_grid(theta, [batch_size,1,self.glimpse_size,self.glimpse_size])\n",
    "                glimpse = F.grid_sample(input_tensor, grid)\n",
    "\n",
    "\n",
    "        ## saving some for output\n",
    "        self.glimpse_output = glimpse\n",
    "        self.input_tensor = input_tensor\n",
    "\n",
    "        return theta_array, probs_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(labels, predicts):\n",
    "    avg_predicts = torch.mean(predicts,0)\n",
    "    pred = avg_predicts.max(1, keepdim=True)[1]\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().item()\n",
    "    return correct\n",
    "\n",
    "\n",
    "\n",
    "def WhereLoss(true_locations, loc_array, glimpse_num = 6):\n",
    "    # Note: shape of loc_array is 6 (num_glimpse) * 128 * 6 (number of parameter)\n",
    "    loss_sum = 0\n",
    "    # glimpse = loc_array[-1]\n",
    "    alpha = torch.tensor([[1.], [0.5], [1.], [0.5], [1.], [1.]])\n",
    "    alpha = alpha.to(device)\n",
    "\n",
    "    for glimpse in loc_array: # glimpse is a 128 * 6 tensor\n",
    "        diff = (torch.sub(glimpse, true_locations)) ** 2\n",
    "        loss = torch.mm(diff, alpha)\n",
    "        loss = 1.0 * torch.mean(loss) # average over number of batch (128 here)\n",
    "        loss_sum += loss\n",
    "    return loss_sum / glimpse_num # average over number of glimpse\n",
    "\n",
    "\n",
    "def AverageLoss(labels, probs_array):\n",
    "    loss_sum = None\n",
    "    for probs in probs_array:\n",
    "        loss = F.nll_loss(probs, labels)\n",
    "        loss = torch.unsqueeze(loss,0)\n",
    "        if loss_sum is None:\n",
    "            loss_sum = loss\n",
    "        else:\n",
    "            loss_sum = torch.cat((loss_sum, loss), 0)\n",
    "    return torch.mean(loss_sum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EDRAM()\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay = 0.05)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        why_loss_all = 0\n",
    "        loc_result = None\n",
    "        class_result = None\n",
    "        feat_result = None\n",
    "        lab_result = None\n",
    "        for feat, lab in test_loader:\n",
    "            lab = lab.long()\n",
    "            feat = feat.to(device)\n",
    "            lab = lab.to(device)\n",
    "            predicted_locations, preds = model(feat)\n",
    "            why_loss = AverageLoss(lab, preds)\n",
    "            loss = why_loss \n",
    "            test_loss += loss\n",
    "            correct += accuracy(lab, preds)\n",
    "            if loc_result is None and class_result is None and feat_result is None and lab_result is None:\n",
    "                loc_result = predicted_locations\n",
    "                class_result = preds\n",
    "                feat_result = feat\n",
    "                lab_result = lab\n",
    "            else:\n",
    "                loc_result = torch.cat((loc_result, predicted_locations),0)\n",
    "                class_result = torch.cat((class_result, preds),0)\n",
    "                feat_result = torch.cat((feat_result, feat),0)\n",
    "                lab_result = torch.cat((lab_result, lab),0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        loc_result = loc_result.cpu().detach().numpy()\n",
    "        class_result = class_result.cpu().detach().numpy()\n",
    "        feat_result = feat_result.cpu().detach().numpy()\n",
    "        lab_result = lab_result.cpu().detach().numpy()\n",
    "        \n",
    "        print('\\nTest set: Why loss: {:.4f}, Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'\n",
    "      .format(why_loss_all, test_loss, correct, len(test_loader.dataset),\n",
    "              100. * correct / len(test_loader.dataset)))\n",
    "        if(correct >= len(test_loader.dataset) * 0.76):\n",
    "            print('=======================================')\n",
    "            print('the number of correct sample: ', correct)\n",
    "            print('SAVING THE RESULTS NOW...')\n",
    "            with open('loc_results_CIFAR.pickle', 'wb') as handle:\n",
    "                pickle.dump(loc_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('class_result_CIFAR.pickle', 'wb') as handle:\n",
    "                print('shape of class result', class_result.shape)\n",
    "                pickle.dump(class_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('feat_result_CIFAR.pickle', 'wb') as handle:\n",
    "                pickle.dump(feat_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('lab_result_CIFAR.pickle', 'wb') as handle:\n",
    "                pickle.dump(lab_result, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            print('SAVE COMPLETED')\n",
    "            print('=======================================')\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (feat, lab) in enumerate(train_loader):\n",
    "        lab = lab.long()\n",
    "        feat = feat.to(device)\n",
    "        lab = lab.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_locations, preds = model(feat)\n",
    "        why_loss = AverageLoss(lab, preds)\n",
    "        loss = why_loss \n",
    "        correct += accuracy(lab, preds)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\"Train Epoch: {} [{} / {} ({:.0f}%)]\\tLoss:{:.6f}\".format(\n",
    "                  epoch, batch_idx * len(feat),\n",
    "              len(train_loader.dataset),\n",
    "              100.* batch_idx / len(train_loader), loss.item()/128))\n",
    "            print('training accuracy: ', correct / 1280)\n",
    "            correct = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0 / 50000 (0%)]\tLoss:0.017957\n",
      "training accuracy:  0.01328125\n",
      "Train Epoch: 0 [1280 / 50000 (3%)]\tLoss:0.017392\n",
      "training accuracy:  0.17421875\n",
      "Train Epoch: 0 [2560 / 50000 (5%)]\tLoss:0.016326\n",
      "training accuracy:  0.26640625\n",
      "Train Epoch: 0 [3840 / 50000 (8%)]\tLoss:0.015303\n",
      "training accuracy:  0.31171875\n",
      "Train Epoch: 0 [5120 / 50000 (10%)]\tLoss:0.015415\n",
      "training accuracy:  0.35625\n",
      "Train Epoch: 0 [6400 / 50000 (13%)]\tLoss:0.014793\n",
      "training accuracy:  0.3734375\n",
      "Train Epoch: 0 [7680 / 50000 (15%)]\tLoss:0.014452\n",
      "training accuracy:  0.3796875\n",
      "Train Epoch: 0 [8960 / 50000 (18%)]\tLoss:0.013264\n",
      "training accuracy:  0.4203125\n",
      "Train Epoch: 0 [10240 / 50000 (21%)]\tLoss:0.014495\n",
      "training accuracy:  0.43515625\n",
      "Train Epoch: 0 [11520 / 50000 (23%)]\tLoss:0.013461\n",
      "training accuracy:  0.43203125\n",
      "Train Epoch: 0 [12800 / 50000 (26%)]\tLoss:0.013349\n",
      "training accuracy:  0.48125\n",
      "Train Epoch: 0 [14080 / 50000 (28%)]\tLoss:0.012972\n",
      "training accuracy:  0.48984375\n",
      "Train Epoch: 0 [15360 / 50000 (31%)]\tLoss:0.012638\n",
      "training accuracy:  0.4859375\n",
      "Train Epoch: 0 [16640 / 50000 (33%)]\tLoss:0.012428\n",
      "training accuracy:  0.48671875\n",
      "Train Epoch: 0 [17920 / 50000 (36%)]\tLoss:0.012589\n",
      "training accuracy:  0.49921875\n",
      "Train Epoch: 0 [19200 / 50000 (38%)]\tLoss:0.011811\n",
      "training accuracy:  0.51171875\n",
      "Train Epoch: 0 [20480 / 50000 (41%)]\tLoss:0.012622\n",
      "training accuracy:  0.49296875\n",
      "Train Epoch: 0 [21760 / 50000 (44%)]\tLoss:0.012392\n",
      "training accuracy:  0.4703125\n",
      "Train Epoch: 0 [23040 / 50000 (46%)]\tLoss:0.012208\n",
      "training accuracy:  0.5\n",
      "Train Epoch: 0 [24320 / 50000 (49%)]\tLoss:0.011931\n",
      "training accuracy:  0.5171875\n",
      "Train Epoch: 0 [25600 / 50000 (51%)]\tLoss:0.011866\n",
      "training accuracy:  0.53046875\n",
      "Train Epoch: 0 [26880 / 50000 (54%)]\tLoss:0.012023\n",
      "training accuracy:  0.53515625\n",
      "Train Epoch: 0 [28160 / 50000 (56%)]\tLoss:0.011625\n",
      "training accuracy:  0.525\n",
      "Train Epoch: 0 [29440 / 50000 (59%)]\tLoss:0.011269\n",
      "training accuracy:  0.52109375\n",
      "Train Epoch: 0 [30720 / 50000 (62%)]\tLoss:0.011325\n",
      "training accuracy:  0.55546875\n",
      "Train Epoch: 0 [32000 / 50000 (64%)]\tLoss:0.010999\n",
      "training accuracy:  0.55390625\n",
      "Train Epoch: 0 [33280 / 50000 (67%)]\tLoss:0.010769\n",
      "training accuracy:  0.55234375\n",
      "Train Epoch: 0 [34560 / 50000 (69%)]\tLoss:0.010913\n",
      "training accuracy:  0.571875\n",
      "Train Epoch: 0 [35840 / 50000 (72%)]\tLoss:0.011551\n",
      "training accuracy:  0.553125\n",
      "Train Epoch: 0 [37120 / 50000 (74%)]\tLoss:0.011836\n",
      "training accuracy:  0.56171875\n",
      "Train Epoch: 0 [38400 / 50000 (77%)]\tLoss:0.010591\n",
      "training accuracy:  0.55625\n",
      "Train Epoch: 0 [39680 / 50000 (79%)]\tLoss:0.009848\n",
      "training accuracy:  0.5671875\n",
      "Train Epoch: 0 [40960 / 50000 (82%)]\tLoss:0.011074\n",
      "training accuracy:  0.5765625\n",
      "Train Epoch: 0 [42240 / 50000 (85%)]\tLoss:0.010126\n",
      "training accuracy:  0.6078125\n",
      "Train Epoch: 0 [43520 / 50000 (87%)]\tLoss:0.010512\n",
      "training accuracy:  0.56640625\n",
      "Train Epoch: 0 [44800 / 50000 (90%)]\tLoss:0.010567\n",
      "training accuracy:  0.5703125\n",
      "Train Epoch: 0 [46080 / 50000 (92%)]\tLoss:0.010560\n",
      "training accuracy:  0.5921875\n",
      "Train Epoch: 0 [47360 / 50000 (95%)]\tLoss:0.010299\n",
      "training accuracy:  0.58046875\n",
      "Train Epoch: 0 [48640 / 50000 (97%)]\tLoss:0.009936\n",
      "training accuracy:  0.603125\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0101, Accuracy: 5820/10000 (58%)\n",
      "\n",
      "Train Epoch: 1 [0 / 50000 (0%)]\tLoss:0.009837\n",
      "training accuracy:  0.06484375\n",
      "Train Epoch: 1 [1280 / 50000 (3%)]\tLoss:0.009604\n",
      "training accuracy:  0.63515625\n",
      "Train Epoch: 1 [2560 / 50000 (5%)]\tLoss:0.009087\n",
      "training accuracy:  0.63671875\n",
      "Train Epoch: 1 [3840 / 50000 (8%)]\tLoss:0.009778\n",
      "training accuracy:  0.62578125\n",
      "Train Epoch: 1 [5120 / 50000 (10%)]\tLoss:0.009839\n",
      "training accuracy:  0.64296875\n",
      "Train Epoch: 1 [6400 / 50000 (13%)]\tLoss:0.009814\n",
      "training accuracy:  0.63359375\n",
      "Train Epoch: 1 [7680 / 50000 (15%)]\tLoss:0.009510\n",
      "training accuracy:  0.6328125\n",
      "Train Epoch: 1 [8960 / 50000 (18%)]\tLoss:0.009360\n",
      "training accuracy:  0.62578125\n",
      "Train Epoch: 1 [10240 / 50000 (21%)]\tLoss:0.008643\n",
      "training accuracy:  0.64453125\n",
      "Train Epoch: 1 [11520 / 50000 (23%)]\tLoss:0.009396\n",
      "training accuracy:  0.63203125\n",
      "Train Epoch: 1 [12800 / 50000 (26%)]\tLoss:0.009161\n",
      "training accuracy:  0.6515625\n",
      "Train Epoch: 1 [14080 / 50000 (28%)]\tLoss:0.009507\n",
      "training accuracy:  0.640625\n",
      "Train Epoch: 1 [15360 / 50000 (31%)]\tLoss:0.009283\n",
      "training accuracy:  0.64453125\n",
      "Train Epoch: 1 [16640 / 50000 (33%)]\tLoss:0.008727\n",
      "training accuracy:  0.65234375\n",
      "Train Epoch: 1 [17920 / 50000 (36%)]\tLoss:0.008084\n",
      "training accuracy:  0.653125\n",
      "Train Epoch: 1 [19200 / 50000 (38%)]\tLoss:0.009217\n",
      "training accuracy:  0.665625\n",
      "Train Epoch: 1 [20480 / 50000 (41%)]\tLoss:0.010623\n",
      "training accuracy:  0.6484375\n",
      "Train Epoch: 1 [21760 / 50000 (44%)]\tLoss:0.009412\n",
      "training accuracy:  0.63359375\n",
      "Train Epoch: 1 [23040 / 50000 (46%)]\tLoss:0.009145\n",
      "training accuracy:  0.66328125\n",
      "Train Epoch: 1 [24320 / 50000 (49%)]\tLoss:0.009020\n",
      "training accuracy:  0.63671875\n",
      "Train Epoch: 1 [25600 / 50000 (51%)]\tLoss:0.008736\n",
      "training accuracy:  0.63671875\n",
      "Train Epoch: 1 [26880 / 50000 (54%)]\tLoss:0.009776\n",
      "training accuracy:  0.65078125\n",
      "Train Epoch: 1 [28160 / 50000 (56%)]\tLoss:0.009132\n",
      "training accuracy:  0.63671875\n",
      "Train Epoch: 1 [29440 / 50000 (59%)]\tLoss:0.010407\n",
      "training accuracy:  0.64140625\n",
      "Train Epoch: 1 [30720 / 50000 (62%)]\tLoss:0.008542\n",
      "training accuracy:  0.65\n",
      "Train Epoch: 1 [32000 / 50000 (64%)]\tLoss:0.009327\n",
      "training accuracy:  0.67734375\n",
      "Train Epoch: 1 [33280 / 50000 (67%)]\tLoss:0.009095\n",
      "training accuracy:  0.6484375\n",
      "Train Epoch: 1 [34560 / 50000 (69%)]\tLoss:0.009552\n",
      "training accuracy:  0.6328125\n",
      "Train Epoch: 1 [35840 / 50000 (72%)]\tLoss:0.009317\n",
      "training accuracy:  0.66484375\n",
      "Train Epoch: 1 [37120 / 50000 (74%)]\tLoss:0.008843\n",
      "training accuracy:  0.6828125\n",
      "Train Epoch: 1 [38400 / 50000 (77%)]\tLoss:0.008491\n",
      "training accuracy:  0.6953125\n",
      "Train Epoch: 1 [39680 / 50000 (79%)]\tLoss:0.009495\n",
      "training accuracy:  0.6375\n",
      "Train Epoch: 1 [40960 / 50000 (82%)]\tLoss:0.008770\n",
      "training accuracy:  0.665625\n",
      "Train Epoch: 1 [42240 / 50000 (85%)]\tLoss:0.008226\n",
      "training accuracy:  0.69609375\n",
      "Train Epoch: 1 [43520 / 50000 (87%)]\tLoss:0.008528\n",
      "training accuracy:  0.65703125\n",
      "Train Epoch: 1 [44800 / 50000 (90%)]\tLoss:0.008873\n",
      "training accuracy:  0.64609375\n",
      "Train Epoch: 1 [46080 / 50000 (92%)]\tLoss:0.008682\n",
      "training accuracy:  0.6640625\n",
      "Train Epoch: 1 [47360 / 50000 (95%)]\tLoss:0.009035\n",
      "training accuracy:  0.65234375\n",
      "Train Epoch: 1 [48640 / 50000 (97%)]\tLoss:0.007878\n",
      "training accuracy:  0.678125\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0090, Accuracy: 6295/10000 (63%)\n",
      "\n",
      "Train Epoch: 2 [0 / 50000 (0%)]\tLoss:0.008040\n",
      "training accuracy:  0.06796875\n",
      "Train Epoch: 2 [1280 / 50000 (3%)]\tLoss:0.008572\n",
      "training accuracy:  0.68984375\n",
      "Train Epoch: 2 [2560 / 50000 (5%)]\tLoss:0.008848\n",
      "training accuracy:  0.70859375\n",
      "Train Epoch: 2 [3840 / 50000 (8%)]\tLoss:0.008393\n",
      "training accuracy:  0.70390625\n",
      "Train Epoch: 2 [5120 / 50000 (10%)]\tLoss:0.007350\n",
      "training accuracy:  0.715625\n",
      "Train Epoch: 2 [6400 / 50000 (13%)]\tLoss:0.008027\n",
      "training accuracy:  0.7375\n",
      "Train Epoch: 2 [7680 / 50000 (15%)]\tLoss:0.008223\n",
      "training accuracy:  0.71796875\n",
      "Train Epoch: 2 [8960 / 50000 (18%)]\tLoss:0.008213\n",
      "training accuracy:  0.72265625\n",
      "Train Epoch: 2 [10240 / 50000 (21%)]\tLoss:0.007837\n",
      "training accuracy:  0.740625\n",
      "Train Epoch: 2 [11520 / 50000 (23%)]\tLoss:0.007538\n",
      "training accuracy:  0.709375\n",
      "Train Epoch: 2 [12800 / 50000 (26%)]\tLoss:0.008816\n",
      "training accuracy:  0.709375\n",
      "Train Epoch: 2 [14080 / 50000 (28%)]\tLoss:0.008053\n",
      "training accuracy:  0.7296875\n",
      "Train Epoch: 2 [15360 / 50000 (31%)]\tLoss:0.008612\n",
      "training accuracy:  0.71171875\n",
      "Train Epoch: 2 [16640 / 50000 (33%)]\tLoss:0.006417\n",
      "training accuracy:  0.7234375\n",
      "Train Epoch: 2 [17920 / 50000 (36%)]\tLoss:0.007310\n",
      "training accuracy:  0.7171875\n",
      "Train Epoch: 2 [19200 / 50000 (38%)]\tLoss:0.008252\n",
      "training accuracy:  0.72734375\n",
      "Train Epoch: 2 [20480 / 50000 (41%)]\tLoss:0.008368\n",
      "training accuracy:  0.703125\n",
      "Train Epoch: 2 [21760 / 50000 (44%)]\tLoss:0.007781\n",
      "training accuracy:  0.71796875\n",
      "Train Epoch: 2 [23040 / 50000 (46%)]\tLoss:0.008577\n",
      "training accuracy:  0.7046875\n",
      "Train Epoch: 2 [24320 / 50000 (49%)]\tLoss:0.007977\n",
      "training accuracy:  0.71640625\n",
      "Train Epoch: 2 [25600 / 50000 (51%)]\tLoss:0.006988\n",
      "training accuracy:  0.72734375\n",
      "Train Epoch: 2 [26880 / 50000 (54%)]\tLoss:0.007370\n",
      "training accuracy:  0.72421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [28160 / 50000 (56%)]\tLoss:0.007005\n",
      "training accuracy:  0.7375\n",
      "Train Epoch: 2 [29440 / 50000 (59%)]\tLoss:0.007644\n",
      "training accuracy:  0.6953125\n",
      "Train Epoch: 2 [30720 / 50000 (62%)]\tLoss:0.007497\n",
      "training accuracy:  0.70859375\n",
      "Train Epoch: 2 [32000 / 50000 (64%)]\tLoss:0.008416\n",
      "training accuracy:  0.7078125\n",
      "Train Epoch: 2 [33280 / 50000 (67%)]\tLoss:0.006995\n",
      "training accuracy:  0.7328125\n",
      "Train Epoch: 2 [34560 / 50000 (69%)]\tLoss:0.008478\n",
      "training accuracy:  0.709375\n",
      "Train Epoch: 2 [35840 / 50000 (72%)]\tLoss:0.008173\n",
      "training accuracy:  0.70390625\n",
      "Train Epoch: 2 [37120 / 50000 (74%)]\tLoss:0.008316\n",
      "training accuracy:  0.70625\n",
      "Train Epoch: 2 [38400 / 50000 (77%)]\tLoss:0.007429\n",
      "training accuracy:  0.725\n",
      "Train Epoch: 2 [39680 / 50000 (79%)]\tLoss:0.006965\n",
      "training accuracy:  0.725\n",
      "Train Epoch: 2 [40960 / 50000 (82%)]\tLoss:0.006976\n",
      "training accuracy:  0.715625\n",
      "Train Epoch: 2 [42240 / 50000 (85%)]\tLoss:0.007535\n",
      "training accuracy:  0.71640625\n",
      "Train Epoch: 2 [43520 / 50000 (87%)]\tLoss:0.007933\n",
      "training accuracy:  0.70859375\n",
      "Train Epoch: 2 [44800 / 50000 (90%)]\tLoss:0.007642\n",
      "training accuracy:  0.69453125\n",
      "Train Epoch: 2 [46080 / 50000 (92%)]\tLoss:0.008185\n",
      "training accuracy:  0.715625\n",
      "Train Epoch: 2 [47360 / 50000 (95%)]\tLoss:0.007250\n",
      "training accuracy:  0.7015625\n",
      "Train Epoch: 2 [48640 / 50000 (97%)]\tLoss:0.006999\n",
      "training accuracy:  0.703125\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0078, Accuracy: 6850/10000 (68%)\n",
      "\n",
      "Train Epoch: 3 [0 / 50000 (0%)]\tLoss:0.007503\n",
      "training accuracy:  0.06953125\n",
      "Train Epoch: 3 [1280 / 50000 (3%)]\tLoss:0.006395\n",
      "training accuracy:  0.75625\n",
      "Train Epoch: 3 [2560 / 50000 (5%)]\tLoss:0.006707\n",
      "training accuracy:  0.771875\n",
      "Train Epoch: 3 [3840 / 50000 (8%)]\tLoss:0.007158\n",
      "training accuracy:  0.7703125\n",
      "Train Epoch: 3 [5120 / 50000 (10%)]\tLoss:0.006441\n",
      "training accuracy:  0.77265625\n",
      "Train Epoch: 3 [6400 / 50000 (13%)]\tLoss:0.007419\n",
      "training accuracy:  0.7546875\n",
      "Train Epoch: 3 [7680 / 50000 (15%)]\tLoss:0.007363\n",
      "training accuracy:  0.75\n",
      "Train Epoch: 3 [8960 / 50000 (18%)]\tLoss:0.006003\n",
      "training accuracy:  0.77265625\n",
      "Train Epoch: 3 [10240 / 50000 (21%)]\tLoss:0.007549\n",
      "training accuracy:  0.759375\n",
      "Train Epoch: 3 [11520 / 50000 (23%)]\tLoss:0.006599\n",
      "training accuracy:  0.77109375\n",
      "Train Epoch: 3 [12800 / 50000 (26%)]\tLoss:0.004880\n",
      "training accuracy:  0.7671875\n",
      "Train Epoch: 3 [14080 / 50000 (28%)]\tLoss:0.007100\n",
      "training accuracy:  0.7546875\n",
      "Train Epoch: 3 [15360 / 50000 (31%)]\tLoss:0.005660\n",
      "training accuracy:  0.74375\n",
      "Train Epoch: 3 [16640 / 50000 (33%)]\tLoss:0.006988\n",
      "training accuracy:  0.74296875\n",
      "Train Epoch: 3 [17920 / 50000 (36%)]\tLoss:0.005829\n",
      "training accuracy:  0.7625\n",
      "Train Epoch: 3 [19200 / 50000 (38%)]\tLoss:0.006380\n",
      "training accuracy:  0.7578125\n",
      "Train Epoch: 3 [20480 / 50000 (41%)]\tLoss:0.007125\n",
      "training accuracy:  0.74296875\n",
      "Train Epoch: 3 [21760 / 50000 (44%)]\tLoss:0.006413\n",
      "training accuracy:  0.75546875\n",
      "Train Epoch: 3 [23040 / 50000 (46%)]\tLoss:0.006805\n",
      "training accuracy:  0.75625\n",
      "Train Epoch: 3 [24320 / 50000 (49%)]\tLoss:0.006220\n",
      "training accuracy:  0.746875\n",
      "Train Epoch: 3 [25600 / 50000 (51%)]\tLoss:0.006608\n",
      "training accuracy:  0.78203125\n",
      "Train Epoch: 3 [26880 / 50000 (54%)]\tLoss:0.006633\n",
      "training accuracy:  0.753125\n",
      "Train Epoch: 3 [28160 / 50000 (56%)]\tLoss:0.006574\n",
      "training accuracy:  0.7671875\n",
      "Train Epoch: 3 [29440 / 50000 (59%)]\tLoss:0.006709\n",
      "training accuracy:  0.753125\n",
      "Train Epoch: 3 [30720 / 50000 (62%)]\tLoss:0.007469\n",
      "training accuracy:  0.765625\n",
      "Train Epoch: 3 [32000 / 50000 (64%)]\tLoss:0.006227\n",
      "training accuracy:  0.76875\n",
      "Train Epoch: 3 [33280 / 50000 (67%)]\tLoss:0.005622\n",
      "training accuracy:  0.765625\n",
      "Train Epoch: 3 [34560 / 50000 (69%)]\tLoss:0.006882\n",
      "training accuracy:  0.74375\n",
      "Train Epoch: 3 [35840 / 50000 (72%)]\tLoss:0.006324\n",
      "training accuracy:  0.7703125\n",
      "Train Epoch: 3 [37120 / 50000 (74%)]\tLoss:0.007079\n",
      "training accuracy:  0.73984375\n",
      "Train Epoch: 3 [38400 / 50000 (77%)]\tLoss:0.007234\n",
      "training accuracy:  0.7515625\n",
      "Train Epoch: 3 [39680 / 50000 (79%)]\tLoss:0.007061\n",
      "training accuracy:  0.740625\n",
      "Train Epoch: 3 [40960 / 50000 (82%)]\tLoss:0.007547\n",
      "training accuracy:  0.74609375\n",
      "Train Epoch: 3 [42240 / 50000 (85%)]\tLoss:0.007020\n",
      "training accuracy:  0.75\n",
      "Train Epoch: 3 [43520 / 50000 (87%)]\tLoss:0.006381\n",
      "training accuracy:  0.75859375\n",
      "Train Epoch: 3 [44800 / 50000 (90%)]\tLoss:0.006052\n",
      "training accuracy:  0.778125\n",
      "Train Epoch: 3 [46080 / 50000 (92%)]\tLoss:0.006948\n",
      "training accuracy:  0.7296875\n",
      "Train Epoch: 3 [47360 / 50000 (95%)]\tLoss:0.007052\n",
      "training accuracy:  0.765625\n",
      "Train Epoch: 3 [48640 / 50000 (97%)]\tLoss:0.006897\n",
      "training accuracy:  0.7421875\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0074, Accuracy: 7042/10000 (70%)\n",
      "\n",
      "Train Epoch: 4 [0 / 50000 (0%)]\tLoss:0.006257\n",
      "training accuracy:  0.075\n",
      "Train Epoch: 4 [1280 / 50000 (3%)]\tLoss:0.005762\n",
      "training accuracy:  0.80390625\n",
      "Train Epoch: 4 [2560 / 50000 (5%)]\tLoss:0.005647\n",
      "training accuracy:  0.821875\n",
      "Train Epoch: 4 [3840 / 50000 (8%)]\tLoss:0.005381\n",
      "training accuracy:  0.8\n",
      "Train Epoch: 4 [5120 / 50000 (10%)]\tLoss:0.005948\n",
      "training accuracy:  0.803125\n",
      "Train Epoch: 4 [6400 / 50000 (13%)]\tLoss:0.006431\n",
      "training accuracy:  0.79296875\n",
      "Train Epoch: 4 [7680 / 50000 (15%)]\tLoss:0.005625\n",
      "training accuracy:  0.8046875\n",
      "Train Epoch: 4 [8960 / 50000 (18%)]\tLoss:0.005437\n",
      "training accuracy:  0.83125\n",
      "Train Epoch: 4 [10240 / 50000 (21%)]\tLoss:0.005318\n",
      "training accuracy:  0.80625\n",
      "Train Epoch: 4 [11520 / 50000 (23%)]\tLoss:0.005721\n",
      "training accuracy:  0.80859375\n",
      "Train Epoch: 4 [12800 / 50000 (26%)]\tLoss:0.005760\n",
      "training accuracy:  0.784375\n",
      "Train Epoch: 4 [14080 / 50000 (28%)]\tLoss:0.005615\n",
      "training accuracy:  0.80078125\n",
      "Train Epoch: 4 [15360 / 50000 (31%)]\tLoss:0.006347\n",
      "training accuracy:  0.7953125\n",
      "Train Epoch: 4 [16640 / 50000 (33%)]\tLoss:0.005449\n",
      "training accuracy:  0.78125\n",
      "Train Epoch: 4 [17920 / 50000 (36%)]\tLoss:0.006199\n",
      "training accuracy:  0.78828125\n",
      "Train Epoch: 4 [19200 / 50000 (38%)]\tLoss:0.006449\n",
      "training accuracy:  0.78046875\n",
      "Train Epoch: 4 [20480 / 50000 (41%)]\tLoss:0.006552\n",
      "training accuracy:  0.77265625\n",
      "Train Epoch: 4 [21760 / 50000 (44%)]\tLoss:0.006444\n",
      "training accuracy:  0.76953125\n",
      "Train Epoch: 4 [23040 / 50000 (46%)]\tLoss:0.005813\n",
      "training accuracy:  0.7828125\n",
      "Train Epoch: 4 [24320 / 50000 (49%)]\tLoss:0.006725\n",
      "training accuracy:  0.7828125\n",
      "Train Epoch: 4 [25600 / 50000 (51%)]\tLoss:0.007219\n",
      "training accuracy:  0.77109375\n",
      "Train Epoch: 4 [26880 / 50000 (54%)]\tLoss:0.005825\n",
      "training accuracy:  0.78359375\n",
      "Train Epoch: 4 [28160 / 50000 (56%)]\tLoss:0.006961\n",
      "training accuracy:  0.76953125\n",
      "Train Epoch: 4 [29440 / 50000 (59%)]\tLoss:0.006018\n",
      "training accuracy:  0.78984375\n",
      "Train Epoch: 4 [30720 / 50000 (62%)]\tLoss:0.006415\n",
      "training accuracy:  0.7625\n",
      "Train Epoch: 4 [32000 / 50000 (64%)]\tLoss:0.006429\n",
      "training accuracy:  0.7875\n",
      "Train Epoch: 4 [33280 / 50000 (67%)]\tLoss:0.005825\n",
      "training accuracy:  0.79453125\n",
      "Train Epoch: 4 [34560 / 50000 (69%)]\tLoss:0.006092\n",
      "training accuracy:  0.79296875\n",
      "Train Epoch: 4 [35840 / 50000 (72%)]\tLoss:0.006568\n",
      "training accuracy:  0.78828125\n",
      "Train Epoch: 4 [37120 / 50000 (74%)]\tLoss:0.005654\n",
      "training accuracy:  0.7984375\n",
      "Train Epoch: 4 [38400 / 50000 (77%)]\tLoss:0.006543\n",
      "training accuracy:  0.7953125\n",
      "Train Epoch: 4 [39680 / 50000 (79%)]\tLoss:0.006037\n",
      "training accuracy:  0.7953125\n",
      "Train Epoch: 4 [40960 / 50000 (82%)]\tLoss:0.006779\n",
      "training accuracy:  0.7671875\n",
      "Train Epoch: 4 [42240 / 50000 (85%)]\tLoss:0.005719\n",
      "training accuracy:  0.8015625\n",
      "Train Epoch: 4 [43520 / 50000 (87%)]\tLoss:0.006519\n",
      "training accuracy:  0.775\n",
      "Train Epoch: 4 [44800 / 50000 (90%)]\tLoss:0.005863\n",
      "training accuracy:  0.79453125\n",
      "Train Epoch: 4 [46080 / 50000 (92%)]\tLoss:0.005404\n",
      "training accuracy:  0.78828125\n",
      "Train Epoch: 4 [47360 / 50000 (95%)]\tLoss:0.007111\n",
      "training accuracy:  0.765625\n",
      "Train Epoch: 4 [48640 / 50000 (97%)]\tLoss:0.006557\n",
      "training accuracy:  0.7921875\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0071, Accuracy: 7305/10000 (73%)\n",
      "\n",
      "Train Epoch: 5 [0 / 50000 (0%)]\tLoss:0.005090\n",
      "training accuracy:  0.084375\n",
      "Train Epoch: 5 [1280 / 50000 (3%)]\tLoss:0.005528\n",
      "training accuracy:  0.8296875\n",
      "Train Epoch: 5 [2560 / 50000 (5%)]\tLoss:0.005616\n",
      "training accuracy:  0.81015625\n",
      "Train Epoch: 5 [3840 / 50000 (8%)]\tLoss:0.004869\n",
      "training accuracy:  0.8484375\n",
      "Train Epoch: 5 [5120 / 50000 (10%)]\tLoss:0.006062\n",
      "training accuracy:  0.82421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [6400 / 50000 (13%)]\tLoss:0.004592\n",
      "training accuracy:  0.8390625\n",
      "Train Epoch: 5 [7680 / 50000 (15%)]\tLoss:0.004620\n",
      "training accuracy:  0.8546875\n",
      "Train Epoch: 5 [8960 / 50000 (18%)]\tLoss:0.005118\n",
      "training accuracy:  0.83203125\n",
      "Train Epoch: 5 [10240 / 50000 (21%)]\tLoss:0.005082\n",
      "training accuracy:  0.82578125\n",
      "Train Epoch: 5 [11520 / 50000 (23%)]\tLoss:0.005377\n",
      "training accuracy:  0.8296875\n",
      "Train Epoch: 5 [12800 / 50000 (26%)]\tLoss:0.006384\n",
      "training accuracy:  0.83125\n",
      "Train Epoch: 5 [14080 / 50000 (28%)]\tLoss:0.005869\n",
      "training accuracy:  0.8109375\n",
      "Train Epoch: 5 [15360 / 50000 (31%)]\tLoss:0.005434\n",
      "training accuracy:  0.821875\n",
      "Train Epoch: 5 [16640 / 50000 (33%)]\tLoss:0.006087\n",
      "training accuracy:  0.81875\n",
      "Train Epoch: 5 [17920 / 50000 (36%)]\tLoss:0.004981\n",
      "training accuracy:  0.85078125\n",
      "Train Epoch: 5 [19200 / 50000 (38%)]\tLoss:0.005691\n",
      "training accuracy:  0.83359375\n",
      "Train Epoch: 5 [20480 / 50000 (41%)]\tLoss:0.005107\n",
      "training accuracy:  0.8234375\n",
      "Train Epoch: 5 [21760 / 50000 (44%)]\tLoss:0.005036\n",
      "training accuracy:  0.79921875\n",
      "Train Epoch: 5 [23040 / 50000 (46%)]\tLoss:0.006456\n",
      "training accuracy:  0.79765625\n",
      "Train Epoch: 5 [24320 / 50000 (49%)]\tLoss:0.006605\n",
      "training accuracy:  0.8078125\n",
      "Train Epoch: 5 [25600 / 50000 (51%)]\tLoss:0.006117\n",
      "training accuracy:  0.81015625\n",
      "Train Epoch: 5 [26880 / 50000 (54%)]\tLoss:0.005992\n",
      "training accuracy:  0.80078125\n",
      "Train Epoch: 5 [28160 / 50000 (56%)]\tLoss:0.005051\n",
      "training accuracy:  0.79375\n",
      "Train Epoch: 5 [29440 / 50000 (59%)]\tLoss:0.005935\n",
      "training accuracy:  0.80390625\n",
      "Train Epoch: 5 [30720 / 50000 (62%)]\tLoss:0.005919\n",
      "training accuracy:  0.8\n",
      "Train Epoch: 5 [32000 / 50000 (64%)]\tLoss:0.006288\n",
      "training accuracy:  0.79453125\n",
      "Train Epoch: 5 [33280 / 50000 (67%)]\tLoss:0.005488\n",
      "training accuracy:  0.82578125\n",
      "Train Epoch: 5 [34560 / 50000 (69%)]\tLoss:0.005273\n",
      "training accuracy:  0.809375\n",
      "Train Epoch: 5 [35840 / 50000 (72%)]\tLoss:0.005406\n",
      "training accuracy:  0.80859375\n",
      "Train Epoch: 5 [37120 / 50000 (74%)]\tLoss:0.005616\n",
      "training accuracy:  0.80703125\n",
      "Train Epoch: 5 [38400 / 50000 (77%)]\tLoss:0.005517\n",
      "training accuracy:  0.81875\n",
      "Train Epoch: 5 [39680 / 50000 (79%)]\tLoss:0.004743\n",
      "training accuracy:  0.80703125\n",
      "Train Epoch: 5 [40960 / 50000 (82%)]\tLoss:0.005724\n",
      "training accuracy:  0.790625\n",
      "Train Epoch: 5 [42240 / 50000 (85%)]\tLoss:0.005670\n",
      "training accuracy:  0.7953125\n",
      "Train Epoch: 5 [43520 / 50000 (87%)]\tLoss:0.005621\n",
      "training accuracy:  0.803125\n",
      "Train Epoch: 5 [44800 / 50000 (90%)]\tLoss:0.005308\n",
      "training accuracy:  0.81953125\n",
      "Train Epoch: 5 [46080 / 50000 (92%)]\tLoss:0.005091\n",
      "training accuracy:  0.82109375\n",
      "Train Epoch: 5 [47360 / 50000 (95%)]\tLoss:0.005402\n",
      "training accuracy:  0.834375\n",
      "Train Epoch: 5 [48640 / 50000 (97%)]\tLoss:0.005208\n",
      "training accuracy:  0.8\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0067, Accuracy: 7417/10000 (74%)\n",
      "\n",
      "Train Epoch: 6 [0 / 50000 (0%)]\tLoss:0.005197\n",
      "training accuracy:  0.0890625\n",
      "Train Epoch: 6 [1280 / 50000 (3%)]\tLoss:0.005373\n",
      "training accuracy:  0.84375\n",
      "Train Epoch: 6 [2560 / 50000 (5%)]\tLoss:0.004565\n",
      "training accuracy:  0.85625\n",
      "Train Epoch: 6 [3840 / 50000 (8%)]\tLoss:0.004487\n",
      "training accuracy:  0.85\n",
      "Train Epoch: 6 [5120 / 50000 (10%)]\tLoss:0.004668\n",
      "training accuracy:  0.846875\n",
      "Train Epoch: 6 [6400 / 50000 (13%)]\tLoss:0.005317\n",
      "training accuracy:  0.84609375\n",
      "Train Epoch: 6 [7680 / 50000 (15%)]\tLoss:0.005280\n",
      "training accuracy:  0.85546875\n",
      "Train Epoch: 6 [8960 / 50000 (18%)]\tLoss:0.004837\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 6 [10240 / 50000 (21%)]\tLoss:0.005346\n",
      "training accuracy:  0.84296875\n",
      "Train Epoch: 6 [11520 / 50000 (23%)]\tLoss:0.005322\n",
      "training accuracy:  0.83828125\n",
      "Train Epoch: 6 [12800 / 50000 (26%)]\tLoss:0.005572\n",
      "training accuracy:  0.83125\n",
      "Train Epoch: 6 [14080 / 50000 (28%)]\tLoss:0.005045\n",
      "training accuracy:  0.84453125\n",
      "Train Epoch: 6 [15360 / 50000 (31%)]\tLoss:0.005442\n",
      "training accuracy:  0.83515625\n",
      "Train Epoch: 6 [16640 / 50000 (33%)]\tLoss:0.005612\n",
      "training accuracy:  0.84609375\n",
      "Train Epoch: 6 [17920 / 50000 (36%)]\tLoss:0.004883\n",
      "training accuracy:  0.83046875\n",
      "Train Epoch: 6 [19200 / 50000 (38%)]\tLoss:0.005608\n",
      "training accuracy:  0.825\n",
      "Train Epoch: 6 [20480 / 50000 (41%)]\tLoss:0.004780\n",
      "training accuracy:  0.809375\n",
      "Train Epoch: 6 [21760 / 50000 (44%)]\tLoss:0.004795\n",
      "training accuracy:  0.8609375\n",
      "Train Epoch: 6 [23040 / 50000 (46%)]\tLoss:0.005232\n",
      "training accuracy:  0.8375\n",
      "Train Epoch: 6 [24320 / 50000 (49%)]\tLoss:0.005000\n",
      "training accuracy:  0.85859375\n",
      "Train Epoch: 6 [25600 / 50000 (51%)]\tLoss:0.004559\n",
      "training accuracy:  0.83046875\n",
      "Train Epoch: 6 [26880 / 50000 (54%)]\tLoss:0.005511\n",
      "training accuracy:  0.8375\n",
      "Train Epoch: 6 [28160 / 50000 (56%)]\tLoss:0.005375\n",
      "training accuracy:  0.8421875\n",
      "Train Epoch: 6 [29440 / 50000 (59%)]\tLoss:0.004963\n",
      "training accuracy:  0.834375\n",
      "Train Epoch: 6 [30720 / 50000 (62%)]\tLoss:0.005130\n",
      "training accuracy:  0.821875\n",
      "Train Epoch: 6 [32000 / 50000 (64%)]\tLoss:0.005195\n",
      "training accuracy:  0.83828125\n",
      "Train Epoch: 6 [33280 / 50000 (67%)]\tLoss:0.004910\n",
      "training accuracy:  0.82890625\n",
      "Train Epoch: 6 [34560 / 50000 (69%)]\tLoss:0.005849\n",
      "training accuracy:  0.82890625\n",
      "Train Epoch: 6 [35840 / 50000 (72%)]\tLoss:0.005150\n",
      "training accuracy:  0.815625\n",
      "Train Epoch: 6 [37120 / 50000 (74%)]\tLoss:0.005508\n",
      "training accuracy:  0.81953125\n",
      "Train Epoch: 6 [38400 / 50000 (77%)]\tLoss:0.005006\n",
      "training accuracy:  0.8296875\n",
      "Train Epoch: 6 [39680 / 50000 (79%)]\tLoss:0.005063\n",
      "training accuracy:  0.83828125\n",
      "Train Epoch: 6 [40960 / 50000 (82%)]\tLoss:0.005294\n",
      "training accuracy:  0.83671875\n",
      "Train Epoch: 6 [42240 / 50000 (85%)]\tLoss:0.006072\n",
      "training accuracy:  0.83515625\n",
      "Train Epoch: 6 [43520 / 50000 (87%)]\tLoss:0.005263\n",
      "training accuracy:  0.8140625\n",
      "Train Epoch: 6 [44800 / 50000 (90%)]\tLoss:0.004843\n",
      "training accuracy:  0.840625\n",
      "Train Epoch: 6 [46080 / 50000 (92%)]\tLoss:0.004735\n",
      "training accuracy:  0.82890625\n",
      "Train Epoch: 6 [47360 / 50000 (95%)]\tLoss:0.005301\n",
      "training accuracy:  0.8203125\n",
      "Train Epoch: 6 [48640 / 50000 (97%)]\tLoss:0.005667\n",
      "training accuracy:  0.8125\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0068, Accuracy: 7359/10000 (74%)\n",
      "\n",
      "Train Epoch: 7 [0 / 50000 (0%)]\tLoss:0.004767\n",
      "training accuracy:  0.0859375\n",
      "Train Epoch: 7 [1280 / 50000 (3%)]\tLoss:0.004404\n",
      "training accuracy:  0.840625\n",
      "Train Epoch: 7 [2560 / 50000 (5%)]\tLoss:0.003589\n",
      "training accuracy:  0.87421875\n",
      "Train Epoch: 7 [3840 / 50000 (8%)]\tLoss:0.004894\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 7 [5120 / 50000 (10%)]\tLoss:0.004920\n",
      "training accuracy:  0.88046875\n",
      "Train Epoch: 7 [6400 / 50000 (13%)]\tLoss:0.004929\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 7 [7680 / 50000 (15%)]\tLoss:0.004014\n",
      "training accuracy:  0.87578125\n",
      "Train Epoch: 7 [8960 / 50000 (18%)]\tLoss:0.004810\n",
      "training accuracy:  0.8796875\n",
      "Train Epoch: 7 [10240 / 50000 (21%)]\tLoss:0.004268\n",
      "training accuracy:  0.87578125\n",
      "Train Epoch: 7 [11520 / 50000 (23%)]\tLoss:0.004630\n",
      "training accuracy:  0.8796875\n",
      "Train Epoch: 7 [12800 / 50000 (26%)]\tLoss:0.005241\n",
      "training accuracy:  0.86328125\n",
      "Train Epoch: 7 [14080 / 50000 (28%)]\tLoss:0.004650\n",
      "training accuracy:  0.85625\n",
      "Train Epoch: 7 [15360 / 50000 (31%)]\tLoss:0.005696\n",
      "training accuracy:  0.84140625\n",
      "Train Epoch: 7 [16640 / 50000 (33%)]\tLoss:0.004834\n",
      "training accuracy:  0.846875\n",
      "Train Epoch: 7 [17920 / 50000 (36%)]\tLoss:0.004309\n",
      "training accuracy:  0.853125\n",
      "Train Epoch: 7 [19200 / 50000 (38%)]\tLoss:0.005422\n",
      "training accuracy:  0.86015625\n",
      "Train Epoch: 7 [20480 / 50000 (41%)]\tLoss:0.004158\n",
      "training accuracy:  0.846875\n",
      "Train Epoch: 7 [21760 / 50000 (44%)]\tLoss:0.004401\n",
      "training accuracy:  0.84765625\n",
      "Train Epoch: 7 [23040 / 50000 (46%)]\tLoss:0.005302\n",
      "training accuracy:  0.8671875\n",
      "Train Epoch: 7 [24320 / 50000 (49%)]\tLoss:0.004873\n",
      "training accuracy:  0.85546875\n",
      "Train Epoch: 7 [25600 / 50000 (51%)]\tLoss:0.004617\n",
      "training accuracy:  0.86328125\n",
      "Train Epoch: 7 [26880 / 50000 (54%)]\tLoss:0.004863\n",
      "training accuracy:  0.8671875\n",
      "Train Epoch: 7 [28160 / 50000 (56%)]\tLoss:0.005023\n",
      "training accuracy:  0.8515625\n",
      "Train Epoch: 7 [29440 / 50000 (59%)]\tLoss:0.005332\n",
      "training accuracy:  0.85\n",
      "Train Epoch: 7 [30720 / 50000 (62%)]\tLoss:0.004696\n",
      "training accuracy:  0.859375\n",
      "Train Epoch: 7 [32000 / 50000 (64%)]\tLoss:0.005135\n",
      "training accuracy:  0.83984375\n",
      "Train Epoch: 7 [33280 / 50000 (67%)]\tLoss:0.004058\n",
      "training accuracy:  0.85625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [34560 / 50000 (69%)]\tLoss:0.004926\n",
      "training accuracy:  0.8546875\n",
      "Train Epoch: 7 [35840 / 50000 (72%)]\tLoss:0.004921\n",
      "training accuracy:  0.85234375\n",
      "Train Epoch: 7 [37120 / 50000 (74%)]\tLoss:0.004718\n",
      "training accuracy:  0.83046875\n",
      "Train Epoch: 7 [38400 / 50000 (77%)]\tLoss:0.005217\n",
      "training accuracy:  0.8515625\n",
      "Train Epoch: 7 [39680 / 50000 (79%)]\tLoss:0.004748\n",
      "training accuracy:  0.84296875\n",
      "Train Epoch: 7 [40960 / 50000 (82%)]\tLoss:0.005625\n",
      "training accuracy:  0.8359375\n",
      "Train Epoch: 7 [42240 / 50000 (85%)]\tLoss:0.004188\n",
      "training accuracy:  0.8265625\n",
      "Train Epoch: 7 [43520 / 50000 (87%)]\tLoss:0.004892\n",
      "training accuracy:  0.8421875\n",
      "Train Epoch: 7 [44800 / 50000 (90%)]\tLoss:0.004612\n",
      "training accuracy:  0.83203125\n",
      "Train Epoch: 7 [46080 / 50000 (92%)]\tLoss:0.004732\n",
      "training accuracy:  0.8421875\n",
      "Train Epoch: 7 [47360 / 50000 (95%)]\tLoss:0.005287\n",
      "training accuracy:  0.84375\n",
      "Train Epoch: 7 [48640 / 50000 (97%)]\tLoss:0.005090\n",
      "training accuracy:  0.83046875\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0066, Accuracy: 7450/10000 (74%)\n",
      "\n",
      "Train Epoch: 8 [0 / 50000 (0%)]\tLoss:0.004290\n",
      "training accuracy:  0.0890625\n",
      "Train Epoch: 8 [1280 / 50000 (3%)]\tLoss:0.004255\n",
      "training accuracy:  0.88828125\n",
      "Train Epoch: 8 [2560 / 50000 (5%)]\tLoss:0.005073\n",
      "training accuracy:  0.88203125\n",
      "Train Epoch: 8 [3840 / 50000 (8%)]\tLoss:0.004120\n",
      "training accuracy:  0.89375\n",
      "Train Epoch: 8 [5120 / 50000 (10%)]\tLoss:0.003576\n",
      "training accuracy:  0.9046875\n",
      "Train Epoch: 8 [6400 / 50000 (13%)]\tLoss:0.004319\n",
      "training accuracy:  0.8921875\n",
      "Train Epoch: 8 [7680 / 50000 (15%)]\tLoss:0.004260\n",
      "training accuracy:  0.89296875\n",
      "Train Epoch: 8 [8960 / 50000 (18%)]\tLoss:0.004146\n",
      "training accuracy:  0.89453125\n",
      "Train Epoch: 8 [10240 / 50000 (21%)]\tLoss:0.004188\n",
      "training accuracy:  0.8953125\n",
      "Train Epoch: 8 [11520 / 50000 (23%)]\tLoss:0.004970\n",
      "training accuracy:  0.878125\n",
      "Train Epoch: 8 [12800 / 50000 (26%)]\tLoss:0.003957\n",
      "training accuracy:  0.8875\n",
      "Train Epoch: 8 [14080 / 50000 (28%)]\tLoss:0.003668\n",
      "training accuracy:  0.88046875\n",
      "Train Epoch: 8 [15360 / 50000 (31%)]\tLoss:0.004665\n",
      "training accuracy:  0.8765625\n",
      "Train Epoch: 8 [16640 / 50000 (33%)]\tLoss:0.004724\n",
      "training accuracy:  0.871875\n",
      "Train Epoch: 8 [17920 / 50000 (36%)]\tLoss:0.004701\n",
      "training accuracy:  0.85234375\n",
      "Train Epoch: 8 [19200 / 50000 (38%)]\tLoss:0.004429\n",
      "training accuracy:  0.86484375\n",
      "Train Epoch: 8 [20480 / 50000 (41%)]\tLoss:0.004870\n",
      "training accuracy:  0.8703125\n",
      "Train Epoch: 8 [21760 / 50000 (44%)]\tLoss:0.004012\n",
      "training accuracy:  0.88046875\n",
      "Train Epoch: 8 [23040 / 50000 (46%)]\tLoss:0.003718\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 8 [24320 / 50000 (49%)]\tLoss:0.004449\n",
      "training accuracy:  0.87109375\n",
      "Train Epoch: 8 [25600 / 50000 (51%)]\tLoss:0.003668\n",
      "training accuracy:  0.86484375\n",
      "Train Epoch: 8 [26880 / 50000 (54%)]\tLoss:0.004387\n",
      "training accuracy:  0.871875\n",
      "Train Epoch: 8 [28160 / 50000 (56%)]\tLoss:0.004706\n",
      "training accuracy:  0.8671875\n",
      "Train Epoch: 8 [29440 / 50000 (59%)]\tLoss:0.006009\n",
      "training accuracy:  0.85390625\n",
      "Train Epoch: 8 [30720 / 50000 (62%)]\tLoss:0.004544\n",
      "training accuracy:  0.85546875\n",
      "Train Epoch: 8 [32000 / 50000 (64%)]\tLoss:0.004840\n",
      "training accuracy:  0.865625\n",
      "Train Epoch: 8 [33280 / 50000 (67%)]\tLoss:0.005463\n",
      "training accuracy:  0.8578125\n",
      "Train Epoch: 8 [34560 / 50000 (69%)]\tLoss:0.005055\n",
      "training accuracy:  0.85625\n",
      "Train Epoch: 8 [35840 / 50000 (72%)]\tLoss:0.004323\n",
      "training accuracy:  0.8734375\n",
      "Train Epoch: 8 [37120 / 50000 (74%)]\tLoss:0.004932\n",
      "training accuracy:  0.85390625\n",
      "Train Epoch: 8 [38400 / 50000 (77%)]\tLoss:0.004603\n",
      "training accuracy:  0.865625\n",
      "Train Epoch: 8 [39680 / 50000 (79%)]\tLoss:0.004605\n",
      "training accuracy:  0.859375\n",
      "Train Epoch: 8 [40960 / 50000 (82%)]\tLoss:0.005196\n",
      "training accuracy:  0.8390625\n",
      "Train Epoch: 8 [42240 / 50000 (85%)]\tLoss:0.004382\n",
      "training accuracy:  0.85234375\n",
      "Train Epoch: 8 [43520 / 50000 (87%)]\tLoss:0.005261\n",
      "training accuracy:  0.8484375\n",
      "Train Epoch: 8 [44800 / 50000 (90%)]\tLoss:0.004637\n",
      "training accuracy:  0.8421875\n",
      "Train Epoch: 8 [46080 / 50000 (92%)]\tLoss:0.005122\n",
      "training accuracy:  0.85625\n",
      "Train Epoch: 8 [47360 / 50000 (95%)]\tLoss:0.004506\n",
      "training accuracy:  0.84140625\n",
      "Train Epoch: 8 [48640 / 50000 (97%)]\tLoss:0.004856\n",
      "training accuracy:  0.84296875\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0066, Accuracy: 7501/10000 (75%)\n",
      "\n",
      "Train Epoch: 9 [0 / 50000 (0%)]\tLoss:0.005434\n",
      "training accuracy:  0.084375\n",
      "Train Epoch: 9 [1280 / 50000 (3%)]\tLoss:0.004349\n",
      "training accuracy:  0.890625\n",
      "Train Epoch: 9 [2560 / 50000 (5%)]\tLoss:0.003548\n",
      "training accuracy:  0.9\n",
      "Train Epoch: 9 [3840 / 50000 (8%)]\tLoss:0.004041\n",
      "training accuracy:  0.88515625\n",
      "Train Epoch: 9 [5120 / 50000 (10%)]\tLoss:0.003949\n",
      "training accuracy:  0.91015625\n",
      "Train Epoch: 9 [6400 / 50000 (13%)]\tLoss:0.003678\n",
      "training accuracy:  0.91484375\n",
      "Train Epoch: 9 [7680 / 50000 (15%)]\tLoss:0.004168\n",
      "training accuracy:  0.89921875\n",
      "Train Epoch: 9 [8960 / 50000 (18%)]\tLoss:0.004095\n",
      "training accuracy:  0.8921875\n",
      "Train Epoch: 9 [10240 / 50000 (21%)]\tLoss:0.004382\n",
      "training accuracy:  0.896875\n",
      "Train Epoch: 9 [11520 / 50000 (23%)]\tLoss:0.003577\n",
      "training accuracy:  0.89453125\n",
      "Train Epoch: 9 [12800 / 50000 (26%)]\tLoss:0.004642\n",
      "training accuracy:  0.88515625\n",
      "Train Epoch: 9 [14080 / 50000 (28%)]\tLoss:0.004236\n",
      "training accuracy:  0.90234375\n",
      "Train Epoch: 9 [15360 / 50000 (31%)]\tLoss:0.003703\n",
      "training accuracy:  0.88984375\n",
      "Train Epoch: 9 [16640 / 50000 (33%)]\tLoss:0.004304\n",
      "training accuracy:  0.89296875\n",
      "Train Epoch: 9 [17920 / 50000 (36%)]\tLoss:0.004446\n",
      "training accuracy:  0.88125\n",
      "Train Epoch: 9 [19200 / 50000 (38%)]\tLoss:0.004112\n",
      "training accuracy:  0.8796875\n",
      "Train Epoch: 9 [20480 / 50000 (41%)]\tLoss:0.004954\n",
      "training accuracy:  0.8765625\n",
      "Train Epoch: 9 [21760 / 50000 (44%)]\tLoss:0.006372\n",
      "training accuracy:  0.86171875\n",
      "Train Epoch: 9 [23040 / 50000 (46%)]\tLoss:0.004759\n",
      "training accuracy:  0.8828125\n",
      "Train Epoch: 9 [24320 / 50000 (49%)]\tLoss:0.004462\n",
      "training accuracy:  0.88984375\n",
      "Train Epoch: 9 [25600 / 50000 (51%)]\tLoss:0.004627\n",
      "training accuracy:  0.884375\n",
      "Train Epoch: 9 [26880 / 50000 (54%)]\tLoss:0.004372\n",
      "training accuracy:  0.88359375\n",
      "Train Epoch: 9 [28160 / 50000 (56%)]\tLoss:0.004251\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 9 [29440 / 50000 (59%)]\tLoss:0.004899\n",
      "training accuracy:  0.85390625\n",
      "Train Epoch: 9 [30720 / 50000 (62%)]\tLoss:0.003836\n",
      "training accuracy:  0.8625\n",
      "Train Epoch: 9 [32000 / 50000 (64%)]\tLoss:0.004553\n",
      "training accuracy:  0.87578125\n",
      "Train Epoch: 9 [33280 / 50000 (67%)]\tLoss:0.005365\n",
      "training accuracy:  0.8671875\n",
      "Train Epoch: 9 [34560 / 50000 (69%)]\tLoss:0.004301\n",
      "training accuracy:  0.8828125\n",
      "Train Epoch: 9 [35840 / 50000 (72%)]\tLoss:0.004924\n",
      "training accuracy:  0.884375\n",
      "Train Epoch: 9 [37120 / 50000 (74%)]\tLoss:0.003899\n",
      "training accuracy:  0.8875\n",
      "Train Epoch: 9 [38400 / 50000 (77%)]\tLoss:0.004913\n",
      "training accuracy:  0.87421875\n",
      "Train Epoch: 9 [39680 / 50000 (79%)]\tLoss:0.004675\n",
      "training accuracy:  0.8640625\n",
      "Train Epoch: 9 [40960 / 50000 (82%)]\tLoss:0.005205\n",
      "training accuracy:  0.8578125\n",
      "Train Epoch: 9 [42240 / 50000 (85%)]\tLoss:0.004895\n",
      "training accuracy:  0.86640625\n",
      "Train Epoch: 9 [43520 / 50000 (87%)]\tLoss:0.004631\n",
      "training accuracy:  0.86796875\n",
      "Train Epoch: 9 [44800 / 50000 (90%)]\tLoss:0.004013\n",
      "training accuracy:  0.85859375\n",
      "Train Epoch: 9 [46080 / 50000 (92%)]\tLoss:0.003792\n",
      "training accuracy:  0.88046875\n",
      "Train Epoch: 9 [47360 / 50000 (95%)]\tLoss:0.003896\n",
      "training accuracy:  0.86953125\n",
      "Train Epoch: 9 [48640 / 50000 (97%)]\tLoss:0.004487\n",
      "training accuracy:  0.8796875\n",
      "\n",
      "Test set: Why loss: 0.0000, Average loss: 0.0063, Accuracy: 7664/10000 (77%)\n",
      "\n",
      "=======================================\n",
      "the number of correct sample:  7664\n",
      "SAVING THE RESULTS NOW...\n",
      "shape of class result (468, 128, 10)\n",
      "SAVE COMPLETED\n",
      "=======================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(0, 60):\n",
    "    train(epoch)\n",
    "    if(test() == True):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.patches as patches\n",
    "# from PIL import Image\n",
    "\n",
    "# path = '/data3/imagenet/data_bounding_box/train/n01440764/n01440764_10040.JPEG'\n",
    "# path_loc = '/data3/imagenet/data_bounding_box/train_loc/n01440764/n01440764_10040.xml'\n",
    "# location = load_loc_raw(path_loc)\n",
    "# print(location)\n",
    "\n",
    "\n",
    "\n",
    "# im = np.array(Image.open(path), dtype=np.uint8)\n",
    "\n",
    "# # Create figure and axes\n",
    "# fig,ax = plt.subplots(1)\n",
    "\n",
    "# # Display the image\n",
    "# ax.imshow(im)\n",
    "\n",
    "# # Create a Rectangle patch\n",
    "# rect = patches.Rectangle((location[0],location[1]),location[2],location[3],linewidth=1,edgecolor='r',facecolor='none')\n",
    "\n",
    "# # Add the patch to the Axes\n",
    "# ax.add_patch(rect)\n",
    "\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize data\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
